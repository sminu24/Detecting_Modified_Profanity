{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import openai\n",
    "openai.api_key = ''\n",
    "\n",
    "dataset = load_dataset(\"jeanlee/kmhas_korean_hate_speech\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec86253",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = dataset['train']\n",
    "td"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08441391",
   "metadata": {},
   "source": [
    "## 비속어 사전을 활용하여 비속어가 포함된 문장 선별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwords = fwords = [\n",
    "    \"개년\",\n",
    "    \"ㄱㅅㄲ\",\n",
    "    \"개새끼\",\n",
    "    \"개씨발\",\n",
    "    \"개씹\",\n",
    "    \"개족새\",\n",
    "    \"개좆\",\n",
    "    \"또라이\",\n",
    "    \"미친\",\n",
    "    \"ㅁㅊ\",\n",
    "    \"병신\",\n",
    "    \"ㅂㅅ\",\n",
    "    \"븅신\",\n",
    "    \"븅딱\",\n",
    "    \"빠가\",\n",
    "    \"뻐큐\",\n",
    "    \"새끼\",\n",
    "    \"ㅅㄲ\",\n",
    "    \"십장생\",\n",
    "    \"쌍년\",\n",
    "    \"쌍놈\",\n",
    "    \"씨발\",\n",
    "    \"ㅆㅂ\",\n",
    "    \"ㅅㅂ\",\n",
    "    \"슈발\",\n",
    "    \"시발\",\n",
    "    \"스발\",\n",
    "    \"싸발\",\n",
    "    \"쓰발\"\n",
    "    \"씨부랄\",\n",
    "    \"씹년\",\n",
    "    \"씹새\",\n",
    "    \"씹새끼\",\n",
    "    \"ㅆ새끼\",\n",
    "    \"씹창\",\n",
    "    \"존나\",\n",
    "    \"좆까\",\n",
    "    \"좆망\", \n",
    "    \"좆\",\n",
    "    \"좆밥\",\n",
    "    \"지랄\",\n",
    "    \"ㅈㄹ\",\n",
    "    \"ㅈㄴ\",\n",
    "    \"졸라\",\n",
    "    \"좆같\",\n",
    "    \"ㅈ같\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b1472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "def find_fwords(example):\n",
    "    included_fwords = [fword for fword in fwords if fword in example['text']]\n",
    "    return {'included_fwords': included_fwords}\n",
    "\n",
    "updated_dataset = dataset['train'].map(find_fwords)\n",
    "\n",
    "# filter 함수를 사용하여 특정 fwords를 포함하는 문장 필터링\n",
    "def contains_fword(example):\n",
    "    return len(example['included_fwords']) > 0\n",
    "\n",
    "filtered_dataset = updated_dataset.filter(contains_fword)\n",
    "df= pd.DataFrame(filtered_dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('beomi/KcELECTRA-base-v2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7707ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug = df.copy()\n",
    "df_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_gpt(example):\n",
    "    \n",
    "    encoded = tokenizer.tokenize(example['text'])\n",
    "    example['words'] = encoded\n",
    "    prompt = f'''\n",
    "    레이블은 'O','B-FW','I-FW' 이렇게 세가지가 있어.\n",
    "    'FW'는 비속어를 의미하고 'B'는 비속어가 시작되는 토큰, 'I'는 비속어의 시작을 제외한 토큰, 'O'태그는 비속어가 아닌 토큰을 의미해.\n",
    "    주어진 예시처럼 문장 속 비속어를 찾아서 레이블링 해줘.\n",
    "\n",
    "    ### 예시:\n",
    "    text: ['씨', '##바', '##알', '.', '.', '노무', '##노무', '술', '##프', '##노', '.', '.', '.', '오늘', '저녁', '##은', '꽂', '##등', '##심이', '##다', '##ㅠ', '##ㅜ']\n",
    "    label: ('B-FW', 'I-FW', 'I-FW', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')\n",
    "\n",
    "    text: ['서열', '##1위', '##네', '좆', '##냥', '##이', '찍소리도', '##못함', '##ㅋ']\n",
    "    label: ('O', 'O', 'O', 'B-FW', 'O', 'O', 'O', 'O', 'O')\n",
    "\n",
    "    text: ['오', 'ㄱ', '##ㅊ', '##노', '?', '?', '박', '하나', '심어', '##놓은', '##다음', '##에', '존나', '##패', '##는거']\n",
    "    label: ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FW', 'O', 'O')\n",
    "\n",
    "    text: ['아', '이', '개', '##병', '##신', '##새끼', '##씨', '##발', '##ㅋㅋ']\n",
    "    label: ('O', 'O', 'O', 'B-FW', 'I-FW', 'I-FW', 'B-FW', 'I-FW', 'O')\n",
    "\n",
    "    text: ['쓰', '##벌', '새끼들아', '보지', '##마', '보면서', '왜', '작가', '##들에게', '상처를', '주나']\n",
    "    label: ('B-FW', 'I-FW', 'B-FW', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')\n",
    "    \n",
    "    text:{encoded}\n",
    "    label:\n",
    "    '''\n",
    "    \n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        label = response.choices[0].message['content']\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        label = 'error'\n",
    "    example['label'] = label\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f686ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = df_aug.progress_apply(aug_gpt,axis=1)\n",
    "labeled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ae0f1",
   "metadata": {},
   "source": [
    "### 에러가 난 경우 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_df = labeled_df[labeled_df['label']!='error']\n",
    "correct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff78956",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt= correct_df[['text','label','words']]\n",
    "gpt = gpt.rename(columns= {'text':'content','label':'labels'})\n",
    "gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a44201",
   "metadata": {},
   "source": [
    "### 끝까지 출력되지 않은 경우 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2835e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = gpt[gpt['labels'].str.endswith(')')]\n",
    "gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a654c084",
   "metadata": {},
   "source": [
    "### 레이블의 길이와 토큰 길이가 다른경우 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = gpt[gpt['labels'].apply(lambda x:len(eval(x)))==gpt['words'].apply(lambda x:len(eval(x)))]\n",
    "gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.to_excel('new_aug_gpt.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
